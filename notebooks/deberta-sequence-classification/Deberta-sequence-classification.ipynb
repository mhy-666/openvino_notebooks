{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fbffdd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2d416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 18:35:38.484288: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import openvino.runtime as ov\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac740c0",
   "metadata": {},
   "source": [
    "# Initializing the Model\n",
    "We will use the transformer-based [microsoft/deberta-base-mnli](https://huggingface.co/microsoft/deberta-base-mnli/tree/main) model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b60f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base-mnli\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-base-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69cceb",
   "metadata": {},
   "source": [
    "# Check model(tokenizer and Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8079cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"I don't like it\",\n",
    "                    \"mhy does well\",\n",
    "                    \"my hometown is ordos, it's a nice place\",\n",
    "                    \"he is a great American\"],\n",
    "                   padding=True,\n",
    "                   return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6068a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf0f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   100,   218,    75,   101,    24,     2,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [    1,   119, 11108,   473,   157,     2,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [    1,  4783,  8994,    16, 22474,   366,     6,    24,    18,    10,\n",
       "          2579,   317,     2],\n",
       "        [    1,   700,    16,    10,   372,   470,     2,     0,     0,     0,\n",
       "             0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09c7e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5745,  2.7616, -1.7987],\n",
       "        [-1.9788,  1.6223, -0.1850],\n",
       "        [-1.9979,  4.6572, -2.7053],\n",
       "        [-0.6713,  4.0547, -3.2321]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0788b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ebfd5",
   "metadata": {},
   "source": [
    "# Convert to ONNX\n",
    "## 1. low level\n",
    "torch.onnx enables you to convert model checkpoints to an ONNX graph by the export method. But you have to provide a lot of values like input_names, dynamic_axes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72a4e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:664: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:664: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:704: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  att_span = min(max(query_layer.size(-2), key_layer.size(-2)), self.max_relative_positions)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:725: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  pos_query_layer /= torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:725: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_query_layer /= torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:726: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_layer.size(-2) != key_layer.size(-2):\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:736: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_layer.size(-2) != key_layer.size(-2):\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:121: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  output = input.masked_fill(rmask, torch.tensor(torch.finfo(input.dtype).min))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    tuple(inputs.values()),\n",
    "    f=\"torch-model.onnx\",\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}},\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99336018",
   "metadata": {},
   "source": [
    "## 2. mid level\n",
    "transformers.onnx enables you to convert model checkpoints to an ONNX graph by leveraging configuration objects. That way you don’t have to provide the complex configuration for dynamic_axes etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ddb3ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:664: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:664: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:704: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  att_span = min(max(query_layer.size(-2), key_layer.size(-2)), self.max_relative_positions)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:725: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  pos_query_layer /= torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:725: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_query_layer /= torch.sqrt(torch.tensor(pos_query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:726: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_layer.size(-2) != key_layer.size(-2):\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:736: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_layer.size(-2) != key_layer.size(-2):\n",
      "/Users/mahaoyang/Library/Python/3.9/lib/python/site-packages/transformers/models/deberta/modeling_deberta.py:121: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  output = input.masked_fill(rmask, torch.tensor(torch.finfo(input.dtype).min))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================ Diagnostic Run torch.onnx.export version 2.0.0 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.onnx import FeaturesManager\n",
    "feature = \"sequence-classification\"\n",
    "\n",
    "# load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(\n",
    "    model, feature=feature)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "    preprocessor=tokenizer,\n",
    "    model=model,\n",
    "    config=onnx_config,\n",
    "    opset=13,\n",
    "    output=Path(\"trfs-model.onnx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3da20d",
   "metadata": {},
   "source": [
    "## high level\n",
    "Optimum Inference includes methods to convert vanilla Transformers models to ONNX using the ORTModelForXxx classes. To convert your Transformers model to ONNX you simply have to pass from_transformers=True to the from_pretrained() method and your model will be loaded and converted to ONNX leveraging the transformers.onnx package under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a4071",
   "metadata": {},
   "source": [
    "# Model Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb25594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /Users/mahaoyang/GSOC/openvino_notebooks/notebooks/deberta-sequence-classification/model/microsoft/deberta-base-mnli.xml\n",
      "[ SUCCESS ] BIN file: /Users/mahaoyang/GSOC/openvino_notebooks/notebooks/deberta-sequence-classification/model/microsoft/deberta-base-mnli.bin\n"
     ]
    }
   ],
   "source": [
    "onnx_model = \"torch-model.onnx\"\n",
    "MODEL_DIR = \"model/\"\n",
    "MODEL_DIR = f\"{MODEL_DIR}\"\n",
    "checkpoint = \"microsoft/deberta-base-mnli\"\n",
    "optimizer_command = f'mo \\\n",
    "    --input_model {onnx_model} \\\n",
    "    --output_dir {MODEL_DIR} \\\n",
    "    --model_name {checkpoint} \\\n",
    "    --input input_ids,attention_mask \\\n",
    "    --input_shape \"[1,128],[1,128]\"'\n",
    "! $optimizer_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d7252cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "core = ov.Core()\n",
    "ir_model_xml = str((Path(MODEL_DIR) / checkpoint).with_suffix(\".xml\"))\n",
    "compiled_model = core.compile_model(ir_model_xml)\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7242aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Creating a softmax function that extracts predictions from the IR format's output.\n",
    "    Parameters: Logits array\n",
    "    Returns: Probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3026e",
   "metadata": {},
   "source": [
    "# Inference\n",
    "When using OpenVINO for inference, there are typically two methods: using InferRequest or CompiledModel for inference. Below, we will implement these two inference methods separately. You can also learn how to use them in [OpenVINO™ Runtime API Tutorial](https://docs.openvino.ai/latest/notebooks/002-openvino-api-with-output.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30fdde79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56fda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_withInferRequest(input_text):\n",
    "    \"\"\"\n",
    "    Creating a generic inference function\n",
    "    to read the input and infer the result\n",
    "    into 3 classes: CONTRADICTION, NEUTRAL or ENTAILMENT.\n",
    "    Parameters: Text to be processed\n",
    "    Returns: Label: CONTRADICTION, NEUTRAL or ENTAILMENT.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = dict(input_text)\n",
    "    label = {0: \"CONTRADICTION\",\n",
    "             1: \"NEUTRAL\",\n",
    "             2: \"ENTAILMENT\"}\n",
    "\n",
    "    # Because Port for tensor name token_type_ids was not found, we need to delete the attribute 'token_type_ids'\n",
    "    del inputs['token_type_ids']\n",
    "    result = infer_request.infer(inputs=inputs)\n",
    "    for i in result.values():\n",
    "        probability = np.argmax(softmax(i))\n",
    "        print(\"probability: \", softmax(i))\n",
    "    return label[probability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d94d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_withCompiledModel(input_text):\n",
    "    \"\"\"\n",
    "    Creating a generic inference function\n",
    "    to read the input and infer the result\n",
    "    into 3 classes: CONTRADICTION, NEUTRAL or ENTAILMENT.\n",
    "    Parameters: Text to be processed\n",
    "    Returns: Label: CONTRADICTION, NEUTRAL or ENTAILMENT.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = dict(input_text)\n",
    "\n",
    "    label = {0: \"CONTRADICTION\",\n",
    "             1: \"NEUTRAL\",\n",
    "             2: \"ENTAILMENT\"}\n",
    "    # Because Port for tensor name token_type_ids was not found, we need to delete the attribute 'token_type_ids'\n",
    "    del inputs['token_type_ids']\n",
    "    # using a dictionary, where the key is input tensor name or index\n",
    "    result = compiled_model(inputs)[output_layer]\n",
    "    for i in result:\n",
    "        probability = np.argmax(softmax(i))\n",
    "        print(\"probability: \", softmax(i))\n",
    "    return label[probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae9e09",
   "metadata": {},
   "source": [
    "## for single input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b85745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability:  [4.7756627e-04 7.9052038e-02 9.2047036e-01]\n",
      "Label:  ENTAILMENT\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I love you. I like you.\"\n",
    "result = infer_withCompiledModel(input_text)\n",
    "print(\"Label: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb977f5",
   "metadata": {},
   "source": [
    "##  Read from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b76107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input:  The food was horrible.\n",
      "\n",
      "probability:  [0.01253021 0.97745925 0.0100106 ]\n",
      "Label:  NEUTRAL \n",
      "\n",
      "User Input:  We went because the restaurant had good reviews.\n",
      "probability:  [1.5560762e-03 9.9773127e-01 7.1273925e-04]\n",
      "Label:  NEUTRAL \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/text/food_reviews.txt\", \"r\") as f:\n",
    "    input_text = f.readlines()\n",
    "    for lines in input_text:\n",
    "        print(\"User Input: \", lines)\n",
    "        result = infer_withCompiledModel(lines)\n",
    "        print(\"Label: \", result, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
